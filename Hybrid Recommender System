import pandas as pd
import numpy as np
import streamlit as st
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns
import altair as alt
import plotly.express as px
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse.linalg import svds
from sklearn.metrics import mean_squared_error, mean_absolute_error


font_css = """
<style>
@font-face {
    font-family: 'Vazir';
    src: url('file:///C:\Users\Mehran\Desktop\Project\Vazir-Regular.ttf') format('truetype');
}
html, body, [class*="st-"] {
    font-family: 'Vazir', sans-serif;
}
</style>
"""
st.markdown(font_css, unsafe_allow_html=True)


# Ú¯Ø§Ù… Û±: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„
@st.cache_data
def load_data(file, file_type):
    if file_type == "csv":
        return pd.read_csv(file)
    elif file_type == "excel":
        return pd.read_excel(file)
    else:
        return None


# ØªØ§Ø¨Ø¹ Ø§Ø¬Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ K-Means
def perform_kmeans(df, n_clusters=3):
    numerical_columns = df.select_dtypes(include=['number']).columns.tolist()
    if not numerical_columns:
        st.error("Ù‡ÛŒÚ† ÙˆÛŒÚ˜Ú¯ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")
        return None, None

    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    df['Cluster'] = kmeans.fit_predict(df[numerical_columns])
    return df, kmeans

# ØªØ§Ø¨Ø¹ Ù†Ù…Ø§ÛŒØ´ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ K-Means
def plot_kmeans_clusters(df):
    fig, ax = plt.subplots(figsize=(8,5))
    sns.scatterplot(x=df.iloc[:, 0], y=df.iloc[:, 1], hue=df['Cluster'], palette="viridis", ax=ax)
    ax.set_title("Ù†ØªØ§ÛŒØ¬ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ K-Means")
    st.pyplot(fig)


# Ú¯Ø§Ù… Û²: Ø¨Ø±Ø±Ø³ÛŒ Ùˆ ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
def clean_data(df):
    # Ù†Ù…Ø§ÛŒØ´ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ Ù‚Ø¨Ù„ Ø§Ø² ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ
    st.write("Missing values before cleaning:", df.isnull().sum())

    for col in df.columns:
        if col in ['Average_Rating', 'Number_Of_Ratings']:
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ Ùˆ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ Ø¨Ø§ ØµÙØ±
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        elif col in ['Categories', 'Author_Name']:
            # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ Ø¨Ø§ Ø±Ø´ØªÙ‡ Ø®Ø§Ù„ÛŒ
            df[col] = df[col].fillna('').astype(str)
        else:
            # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
            if df[col].dtype in ["float64", "int64"]:
                df[col].fillna(df[col].mean(), inplace=True)
            else:
                df[col].fillna("Unknown", inplace=True)

    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ù…Ù†Ø§Ø³Ø¨
    if 'Average_Rating' in df.columns:
        df['Average_Rating'] = pd.to_numeric(df['Average_Rating'], errors='coerce').fillna(0)
    if 'Number_Of_Ratings' in df.columns:
        df['Number_Of_Ratings'] = pd.to_numeric(df['Number_Of_Ratings'], errors='coerce').fillna(0)

    # Ù†Ù…Ø§ÛŒØ´ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ Ù¾Ø³ Ø§Ø² ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ
    st.write("Missing values after cleaning:", df.isnull().sum())
    return df

# Ú¯Ø§Ù… Û³: ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
def analyze_data(books, users, ratings):
    # Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø§Ø² Ù‡Ø± ÙØ§ÛŒÙ„
    st.write("Books Info:")
    st.dataframe(books.head())
    st.write("Users Info:")
    st.dataframe(users.head())
    st.write("Ratings Info:")
    st.dataframe(ratings.head())

    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ† 'Average_Rating'
    if 'Average_Rating' not in books.columns:
        st.error("Column 'Average_Rating' not found in dataset!")
        return  

    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¹Ø¯Ø¯ÛŒ Ø¨ÙˆØ¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ±
    books['Average_Rating'] = pd.to_numeric(books['Average_Rating'], errors='coerce')

    # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙÙ‚Ø· Ù…Ù‚Ø§Ø¯ÛŒØ± 1 ØªØ§ 5
    valid_ratings = [1, 2, 3, 4, 5]
    filtered_ratings = books[books['Average_Rating'].isin(valid_ratings)]

    # Ù†Ù…Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±
    st.write(f"Number of valid ratings: {len(filtered_ratings)}")

    # Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
    if not filtered_ratings.empty:
        st.write("Average Rating Distribution (1 to 5 only):")
        fig, ax = plt.subplots()
        sns.histplot(filtered_ratings['Average_Rating'], bins=5, discrete=True, kde=False, ax=ax, color="blue")
        ax.set_xticks(valid_ratings)  # Ù†Ù…Ø§ÛŒØ´ ÙÙ‚Ø· Ø§Ø¹Ø¯Ø§Ø¯ 1 ØªØ§ 5 Ø±ÙˆÛŒ Ù…Ø­ÙˆØ± x
        st.pyplot(fig)
    else:
        st.warning("No valid ratings available to display.")


    # ØªÙˆØ²ÛŒØ¹ ØªØ¹Ø¯Ø§Ø¯ Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§
    st.write("Number of Ratings Distribution:")
    fig, ax = plt.subplots()
    sns.boxplot(books['Number_Of_Ratings'], ax=ax, color="green")
    st.pyplot(fig)

    # Ù†Ù…ÙˆØ¯Ø§Ø± Ú©Ù„Ù…Ø§Øª Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†
    st.write("WordCloud for Authors:")
    wordcloud = WordCloud(
        width=800,
        height=400,
        background_color="white",
        font_path="C:/Users/Mehran/Desktop/Vazir-Regular.ttf"  # Ù…Ø³ÛŒØ± ÙÙˆÙ†Øª ÙØ§Ø±Ø³ÛŒ
    ).generate(" ".join(books['Author_Name']))
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis("off")
    st.pyplot(fig)

# Ú¯Ø§Ù… Û´: Ø³ÛŒØ³ØªÙ… Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø­ØªÙˆØ§
def content_based_recommender(books, book_id):
    tfidf = TfidfVectorizer(stop_words='english')

    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ† 'Categories' Ùˆ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
    if 'Categuries' in books.columns:
        books.rename(columns={'Categuries': 'Categories'}, inplace=True)

    books['Categories'] = books['Categories'].fillna('')
    tfidf_matrix = tfidf.fit_transform(books['Categories'] + " " + books['Author_Name'])
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
    indices = pd.Series(books.index, index=books['Book_ID']).drop_duplicates()

    idx = indices[book_id]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:11]  # Û±Û° Ú©ØªØ§Ø¨ Ù…Ø´Ø§Ø¨Ù‡
    book_indices = [i[0] for i in sim_scores]
    return books.iloc[book_indices]

# Ú¯Ø§Ù… Ûµ: Ø³ÛŒØ³ØªÙ… Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù‡Ù…Ú©Ø§Ø±ÛŒ
def collaborative_filtering_recommender(ratings, books, user_id):
    user_book_matrix = ratings.pivot(index='User_ID', columns='Book_ID', values='Book_Rating').fillna(0)
    user_ratings_mean = np.mean(user_book_matrix, axis=1)
    matrix_normalized = user_book_matrix - user_ratings_mean.values.reshape(-1, 1)

    # ØªØ¨Ø¯ÛŒÙ„ Ù…Ø§ØªØ±ÛŒØ³ Ø¨Ù‡ ÙØ±Ù…Øª Dense
    matrix_normalized = matrix_normalized.values

    # ØªÙ†Ø¸ÛŒÙ… Ù…Ù‚Ø¯Ø§Ø± k
    k = min(matrix_normalized.shape) - 1  # Ù…Ù‚Ø¯Ø§Ø± k Ù†Ø¨Ø§ÛŒØ¯ Ø§Ø² Ø§Ø¨Ø¹Ø§Ø¯ Ù…Ø§ØªØ±ÛŒØ³ Ø¨ÛŒØ´ØªØ± Ø¨Ø§Ø´Ø¯

    U, sigma, Vt = svds(matrix_normalized, k=k)
    sigma = np.diag(sigma)

    predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.values.reshape(-1, 1)
    preds_df = pd.DataFrame(predicted_ratings, columns=user_book_matrix.columns, index=user_book_matrix.index)

    user_row = preds_df.loc[user_id].sort_values(ascending=False)
    recommended_books = books[books['Book_ID'].isin(user_row.head(10).index)]
    return recommended_books

# Ú¯Ø§Ù… Û¶: Ø³ÛŒØ³ØªÙ… ØªØ±Ú©ÛŒØ¨ÛŒ
def hybrid_recommender(books, ratings, user_id):
    # Ø³ÛŒØ³ØªÙ… Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù…Ø­ØªÙˆØ§
    user_rated_books = ratings[ratings['User_ID'] == user_id]['Book_ID']
    content_recommendations = pd.DataFrame()

    for book_id in user_rated_books:
        content_rec = content_based_recommender(books, book_id)
        content_recommendations = pd.concat([content_recommendations, content_rec])

    # Ø³ÛŒØ³ØªÙ… Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù‡Ù…Ú©Ø§Ø±ÛŒ
    collab_rec = collaborative_filtering_recommender(ratings, books, user_id)

    # ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬
    hybrid_rec = pd.concat([content_recommendations, collab_rec]).drop_duplicates().head(10)
    return hybrid_rec

# Ú¯Ø§Ù… Û·: Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ú¯Ø± Ø¨Ø§ Streamlit
def main():
    st.title("Ø³ÛŒØ³ØªÙ… Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ú¯Ø± ØªØ±Ú©ÛŒØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Ú©ØªØ§Ø¨â€ŒÙ‡Ø§")
    st.sidebar.title("Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§")
    books_file = st.sidebar.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Books", type=["csv", "xlsx"])
    users_file = st.sidebar.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Users", type=["csv", "xlsx"])
    ratings_file = st.sidebar.file_uploader("Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ratings", type=["csv", "xlsx"])

    if books_file and users_file and ratings_file:
        books = load_data(books_file, books_file.name.split(".")[-1])
        users = load_data(users_file, users_file.name.split(".")[-1])
        ratings = load_data(ratings_file, ratings_file.name.split(".")[-1])

        metric_selection = st.sidebar.selectbox("Ø§Ù†ØªØ®Ø§Ø¨ Ù…ØªØ±ÛŒÚ© Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:", ["ACSI", "RMSE Ùˆ MAE", "K-Means"])
        
        if metric_selection == "ACSI":
            st.header("Ù†Ø¸Ø±Ø³Ù†Ø¬ÛŒ Ø±Ø¶Ø§ÛŒØªÙ…Ù†Ø¯ÛŒ Ú©Ø§Ø±Ø¨Ø±")
            user_id = st.selectbox("Ø´Ù†Ø§Ø³Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:", users['User_ID'].unique())
            q1 = st.slider("1- Ù…ÛŒØ²Ø§Ù† Ø±Ø¶Ø§ÛŒØª Ø§Ø² Ø³ÛŒØ³ØªÙ…ØŸ", 1, 10, 5)
            q2 = st.slider("2- ÙØ§ØµÙ„Ù‡ Ø§Ø² Ø§ÛŒØ¯Ù‡â€ŒØ¢Ù„ØŸ", 1, 10, 5)
            q3 = st.slider("3- Ø¢ÛŒØ§ Ø³ÛŒØ³ØªÙ… Ø±Ø§ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ØŸ", 1, 10, 5)
            q4 = st.slider("4- Ù‡Ù…Ø®ÙˆØ§Ù†ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ø§ Ø®Ø¯Ù…Ø§ØªØŸ", 1, 10, 5)
            if st.button("Ù…Ø­Ø§Ø³Ø¨Ù‡ ACSI"):
                acsi_score = (q1 * 0.4) + (q2 * 0.3) + (q3 * 0.2) + (q4 * 0.1)
                st.write(f"ğŸ”¹ **Ø´Ø§Ø®Øµ Ø±Ø¶Ø§ÛŒØª Ù…Ø´ØªØ±ÛŒ (ACSI): {acsi_score:.2f} Ø§Ø² 10**")

        elif metric_selection == "RMSE Ùˆ MAE":
            if st.button("Ù…Ø­Ø§Ø³Ø¨Ù‡ RMSE Ùˆ MAE"):
                actual_ratings = ratings['Book_Rating']
                predicted_ratings = ratings['Book_Rating'].apply(lambda x: np.random.uniform(1, 5))
                rmse = np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))
                mae = mean_absolute_error(actual_ratings, predicted_ratings)
                st.write(f"ğŸ“Š **RMSE:** {rmse:.4f}")
                st.write(f"ğŸ“Š **MAE:** {mae:.4f}")
                error_data = pd.DataFrame({"Ù…Ø¹ÛŒØ§Ø±": ["RMSE", "MAE"], "Ù…Ù‚Ø¯Ø§Ø±": [rmse, mae]})
                fig = px.bar(error_data, x="Ù…Ø¹ÛŒØ§Ø±", y="Ù…Ù‚Ø¯Ø§Ø±", text="Ù…Ù‚Ø¯Ø§Ø±", color="Ù…Ù‚Ø¯Ø§Ø±", color_continuous_scale="reds", title="Ù…Ù‚Ø§ÛŒØ³Ù‡ RMSE Ùˆ MAE")
                fig.update_traces(texttemplate='%{text:.4f}', textposition='outside')
                fig.update_layout(yaxis=dict(range=[0, max(rmse, mae) + 0.5]))
                st.plotly_chart(fig)

        elif metric_selection == "K-Means":
            st.header("Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ K-Means")
            num_clusters = st.slider("ØªØ¹Ø¯Ø§Ø¯ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§", 2, 10, 3)
            if st.button("Ø§Ø¬Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ K-Means"):
                df, kmeans = perform_kmeans(books, n_clusters=num_clusters)
                if df is not None:
                    st.write("### Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒØ´Ø¯Ù‡:")
                    st.dataframe(df.head())
                    plot_kmeans_clusters(df)


if __name__ == "__main__":
    main()
